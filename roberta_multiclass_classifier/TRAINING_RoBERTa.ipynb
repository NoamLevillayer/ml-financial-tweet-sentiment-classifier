{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\a\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Label counts:\n",
      " label\n",
      "0     2884\n",
      "1     3846\n",
      "2    12356\n",
      "Name: count, dtype: int64\n",
      "Computed class weights:\n",
      " label\n",
      "0    2.205964\n",
      "1    1.654186\n",
      "2    0.514892\n",
      "Name: count, dtype: float64\n",
      "Tensor class weights: tensor([2.2060, 1.6542, 0.5149])\n",
      "WARNING:tensorflow:From c:\\Users\\a\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Classifier layer shape: torch.Size([3, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19086/19086 [00:03<00:00, 6055.80 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 10-fold Cross-Validation with StratifiedKFold...\n",
      "\n",
      "Starting fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "All TF 2.0 model weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "C:\\Users\\a\\AppData\\Local\\Temp\\ipykernel_5808\\228697118.py:175: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4290' max='4290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4290/4290 35:46, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.445500</td>\n",
       "      <td>0.495441</td>\n",
       "      <td>0.811943</td>\n",
       "      <td>0.849332</td>\n",
       "      <td>0.811943</td>\n",
       "      <td>0.820918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.324600</td>\n",
       "      <td>0.331663</td>\n",
       "      <td>0.876899</td>\n",
       "      <td>0.879987</td>\n",
       "      <td>0.876899</td>\n",
       "      <td>0.877715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.274900</td>\n",
       "      <td>0.335327</td>\n",
       "      <td>0.888423</td>\n",
       "      <td>0.899047</td>\n",
       "      <td>0.888423</td>\n",
       "      <td>0.890573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.197900</td>\n",
       "      <td>0.340052</td>\n",
       "      <td>0.903091</td>\n",
       "      <td>0.913377</td>\n",
       "      <td>0.903091</td>\n",
       "      <td>0.905243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.071600</td>\n",
       "      <td>0.311322</td>\n",
       "      <td>0.933473</td>\n",
       "      <td>0.933073</td>\n",
       "      <td>0.933473</td>\n",
       "      <td>0.933150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>0.332763</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.940771</td>\n",
       "      <td>0.939759</td>\n",
       "      <td>0.940120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.326009</td>\n",
       "      <td>0.941854</td>\n",
       "      <td>0.942928</td>\n",
       "      <td>0.941854</td>\n",
       "      <td>0.942030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>0.339222</td>\n",
       "      <td>0.944474</td>\n",
       "      <td>0.944752</td>\n",
       "      <td>0.944474</td>\n",
       "      <td>0.944589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.345773</td>\n",
       "      <td>0.947093</td>\n",
       "      <td>0.947287</td>\n",
       "      <td>0.947093</td>\n",
       "      <td>0.947158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.346155</td>\n",
       "      <td>0.947617</td>\n",
       "      <td>0.947851</td>\n",
       "      <td>0.947617</td>\n",
       "      <td>0.947700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='239' max='239' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [239/239 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for fold 1: {'eval_loss': 0.3461545705795288, 'eval_accuracy': 0.9476165531691986, 'eval_precision': 0.9478505819056045, 'eval_recall': 0.9476165531691986, 'eval_weighted_f1': 0.9477001468421047, 'eval_runtime': 6.0227, 'eval_samples_per_second': 316.965, 'eval_steps_per_second': 39.683, 'epoch': 9.998603351955307, 'time_elapsed_secs': 2156.668385028839}\n",
      "\n",
      "Starting fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "All TF 2.0 model weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "C:\\Users\\a\\AppData\\Local\\Temp\\ipykernel_5808\\228697118.py:175: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3861' max='4290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3861/4290 1:07:18 < 07:29, 0.96 it/s, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.365900</td>\n",
       "      <td>0.352166</td>\n",
       "      <td>0.872708</td>\n",
       "      <td>0.880702</td>\n",
       "      <td>0.872708</td>\n",
       "      <td>0.875105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.335200</td>\n",
       "      <td>0.300409</td>\n",
       "      <td>0.899948</td>\n",
       "      <td>0.904060</td>\n",
       "      <td>0.899948</td>\n",
       "      <td>0.900922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.195800</td>\n",
       "      <td>0.322024</td>\n",
       "      <td>0.888947</td>\n",
       "      <td>0.900761</td>\n",
       "      <td>0.888947</td>\n",
       "      <td>0.891344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.091900</td>\n",
       "      <td>0.351540</td>\n",
       "      <td>0.910424</td>\n",
       "      <td>0.919317</td>\n",
       "      <td>0.910424</td>\n",
       "      <td>0.912263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.095100</td>\n",
       "      <td>0.305053</td>\n",
       "      <td>0.935045</td>\n",
       "      <td>0.935190</td>\n",
       "      <td>0.935045</td>\n",
       "      <td>0.934600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.060200</td>\n",
       "      <td>0.343808</td>\n",
       "      <td>0.929282</td>\n",
       "      <td>0.931357</td>\n",
       "      <td>0.929282</td>\n",
       "      <td>0.929945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.300830</td>\n",
       "      <td>0.943426</td>\n",
       "      <td>0.943183</td>\n",
       "      <td>0.943426</td>\n",
       "      <td>0.943271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>0.355010</td>\n",
       "      <td>0.942378</td>\n",
       "      <td>0.942641</td>\n",
       "      <td>0.942378</td>\n",
       "      <td>0.942484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>0.353812</td>\n",
       "      <td>0.940807</td>\n",
       "      <td>0.941299</td>\n",
       "      <td>0.940807</td>\n",
       "      <td>0.940985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='239' max='239' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [239/239 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for fold 2: {'eval_loss': 0.30083024501800537, 'eval_accuracy': 0.9434258774227344, 'eval_precision': 0.9431832236309632, 'eval_recall': 0.9434258774227344, 'eval_weighted_f1': 0.9432711235298249, 'eval_runtime': 31.1055, 'eval_samples_per_second': 61.372, 'eval_steps_per_second': 7.684, 'epoch': 8.998603351955307, 'time_elapsed_secs': 4073.7829988002777}\n",
      "\n",
      "Starting fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "All TF 2.0 model weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "C:\\Users\\a\\AppData\\Local\\Temp\\ipykernel_5808\\228697118.py:175: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4290' max='4290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4290/4290 35:25, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.460100</td>\n",
       "      <td>0.433537</td>\n",
       "      <td>0.833944</td>\n",
       "      <td>0.858515</td>\n",
       "      <td>0.833944</td>\n",
       "      <td>0.839237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.260400</td>\n",
       "      <td>0.292305</td>\n",
       "      <td>0.900471</td>\n",
       "      <td>0.903520</td>\n",
       "      <td>0.900471</td>\n",
       "      <td>0.901380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>0.278545</td>\n",
       "      <td>0.906757</td>\n",
       "      <td>0.908610</td>\n",
       "      <td>0.906757</td>\n",
       "      <td>0.907394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.191400</td>\n",
       "      <td>0.261319</td>\n",
       "      <td>0.924044</td>\n",
       "      <td>0.924314</td>\n",
       "      <td>0.924044</td>\n",
       "      <td>0.924155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.305064</td>\n",
       "      <td>0.942378</td>\n",
       "      <td>0.942156</td>\n",
       "      <td>0.942378</td>\n",
       "      <td>0.942163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.049600</td>\n",
       "      <td>0.282355</td>\n",
       "      <td>0.944474</td>\n",
       "      <td>0.944663</td>\n",
       "      <td>0.944474</td>\n",
       "      <td>0.944558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.305370</td>\n",
       "      <td>0.949712</td>\n",
       "      <td>0.949995</td>\n",
       "      <td>0.949712</td>\n",
       "      <td>0.949834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>0.298898</td>\n",
       "      <td>0.951283</td>\n",
       "      <td>0.951016</td>\n",
       "      <td>0.951283</td>\n",
       "      <td>0.950995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.286704</td>\n",
       "      <td>0.953379</td>\n",
       "      <td>0.953103</td>\n",
       "      <td>0.953379</td>\n",
       "      <td>0.953182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.286094</td>\n",
       "      <td>0.954426</td>\n",
       "      <td>0.954169</td>\n",
       "      <td>0.954426</td>\n",
       "      <td>0.954241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='239' max='239' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [239/239 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for fold 3: {'eval_loss': 0.28609412908554077, 'eval_accuracy': 0.9544264012572027, 'eval_precision': 0.9541694699532889, 'eval_recall': 0.9544264012572027, 'eval_weighted_f1': 0.9542410945205343, 'eval_runtime': 5.5486, 'eval_samples_per_second': 344.053, 'eval_steps_per_second': 43.074, 'epoch': 9.998603351955307, 'time_elapsed_secs': 2134.239250898361}\n",
      "\n",
      "Starting fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "All TF 2.0 model weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "C:\\Users\\a\\AppData\\Local\\Temp\\ipykernel_5808\\228697118.py:175: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4290' max='4290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4290/4290 35:31, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.482300</td>\n",
       "      <td>0.439942</td>\n",
       "      <td>0.838659</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.838659</td>\n",
       "      <td>0.843804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.288800</td>\n",
       "      <td>0.299529</td>\n",
       "      <td>0.894185</td>\n",
       "      <td>0.897587</td>\n",
       "      <td>0.894185</td>\n",
       "      <td>0.895314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.216600</td>\n",
       "      <td>0.440011</td>\n",
       "      <td>0.856993</td>\n",
       "      <td>0.887707</td>\n",
       "      <td>0.856993</td>\n",
       "      <td>0.861658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.252000</td>\n",
       "      <td>0.295099</td>\n",
       "      <td>0.910948</td>\n",
       "      <td>0.916729</td>\n",
       "      <td>0.910948</td>\n",
       "      <td>0.912326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.098100</td>\n",
       "      <td>0.337965</td>\n",
       "      <td>0.929282</td>\n",
       "      <td>0.928718</td>\n",
       "      <td>0.929282</td>\n",
       "      <td>0.928795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.029100</td>\n",
       "      <td>0.333169</td>\n",
       "      <td>0.936616</td>\n",
       "      <td>0.936126</td>\n",
       "      <td>0.936616</td>\n",
       "      <td>0.936145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.045500</td>\n",
       "      <td>0.338766</td>\n",
       "      <td>0.936092</td>\n",
       "      <td>0.936325</td>\n",
       "      <td>0.936092</td>\n",
       "      <td>0.936192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.335480</td>\n",
       "      <td>0.942902</td>\n",
       "      <td>0.943409</td>\n",
       "      <td>0.942902</td>\n",
       "      <td>0.943048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.343962</td>\n",
       "      <td>0.948140</td>\n",
       "      <td>0.948129</td>\n",
       "      <td>0.948140</td>\n",
       "      <td>0.948116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.346386</td>\n",
       "      <td>0.946569</td>\n",
       "      <td>0.946646</td>\n",
       "      <td>0.946569</td>\n",
       "      <td>0.946599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='239' max='239' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [239/239 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for fold 4: {'eval_loss': 0.34396159648895264, 'eval_accuracy': 0.9481403876375065, 'eval_precision': 0.9481287323752459, 'eval_recall': 0.9481403876375065, 'eval_weighted_f1': 0.948116189583171, 'eval_runtime': 5.189, 'eval_samples_per_second': 367.896, 'eval_steps_per_second': 46.059, 'epoch': 9.998603351955307, 'time_elapsed_secs': 2139.797264099121}\n",
      "\n",
      "Starting fold 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "All TF 2.0 model weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "C:\\Users\\a\\AppData\\Local\\Temp\\ipykernel_5808\\228697118.py:175: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4290' max='4290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4290/4290 50:22, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.470700</td>\n",
       "      <td>0.428293</td>\n",
       "      <td>0.842326</td>\n",
       "      <td>0.859798</td>\n",
       "      <td>0.842326</td>\n",
       "      <td>0.846381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.369000</td>\n",
       "      <td>0.301847</td>\n",
       "      <td>0.890519</td>\n",
       "      <td>0.900519</td>\n",
       "      <td>0.890519</td>\n",
       "      <td>0.892707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.217400</td>\n",
       "      <td>0.278917</td>\n",
       "      <td>0.918806</td>\n",
       "      <td>0.920859</td>\n",
       "      <td>0.918806</td>\n",
       "      <td>0.919430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.135100</td>\n",
       "      <td>0.269988</td>\n",
       "      <td>0.935045</td>\n",
       "      <td>0.936326</td>\n",
       "      <td>0.935045</td>\n",
       "      <td>0.935397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>0.242363</td>\n",
       "      <td>0.943950</td>\n",
       "      <td>0.944087</td>\n",
       "      <td>0.943950</td>\n",
       "      <td>0.944011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.037900</td>\n",
       "      <td>0.308663</td>\n",
       "      <td>0.943426</td>\n",
       "      <td>0.944827</td>\n",
       "      <td>0.943426</td>\n",
       "      <td>0.943828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.305230</td>\n",
       "      <td>0.944997</td>\n",
       "      <td>0.948063</td>\n",
       "      <td>0.944997</td>\n",
       "      <td>0.945710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.062700</td>\n",
       "      <td>0.287851</td>\n",
       "      <td>0.952855</td>\n",
       "      <td>0.953009</td>\n",
       "      <td>0.952855</td>\n",
       "      <td>0.952921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.302958</td>\n",
       "      <td>0.952331</td>\n",
       "      <td>0.952807</td>\n",
       "      <td>0.952331</td>\n",
       "      <td>0.952489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.298939</td>\n",
       "      <td>0.953379</td>\n",
       "      <td>0.953657</td>\n",
       "      <td>0.953379</td>\n",
       "      <td>0.953478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='239' max='239' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [239/239 00:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for fold 5: {'eval_loss': 0.2989385426044464, 'eval_accuracy': 0.9533787323205867, 'eval_precision': 0.9536566258372823, 'eval_recall': 0.9533787323205867, 'eval_weighted_f1': 0.9534782040715195, 'eval_runtime': 44.9112, 'eval_samples_per_second': 42.506, 'eval_steps_per_second': 5.322, 'epoch': 9.998603351955307, 'time_elapsed_secs': 3073.0223982334137}\n",
      "\n",
      "Starting fold 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "All TF 2.0 model weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "C:\\Users\\a\\AppData\\Local\\Temp\\ipykernel_5808\\228697118.py:175: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3432' max='4290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3432/4290 29:05 < 07:16, 1.96 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.512200</td>\n",
       "      <td>0.410772</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.867416</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.856708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.329100</td>\n",
       "      <td>0.252798</td>\n",
       "      <td>0.911472</td>\n",
       "      <td>0.912136</td>\n",
       "      <td>0.911472</td>\n",
       "      <td>0.911678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.223700</td>\n",
       "      <td>0.282109</td>\n",
       "      <td>0.907281</td>\n",
       "      <td>0.916460</td>\n",
       "      <td>0.907281</td>\n",
       "      <td>0.909251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.117200</td>\n",
       "      <td>0.261542</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.929963</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.928389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.281528</td>\n",
       "      <td>0.933997</td>\n",
       "      <td>0.933560</td>\n",
       "      <td>0.933997</td>\n",
       "      <td>0.933582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>0.281731</td>\n",
       "      <td>0.948140</td>\n",
       "      <td>0.948713</td>\n",
       "      <td>0.948140</td>\n",
       "      <td>0.948239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.308626</td>\n",
       "      <td>0.946045</td>\n",
       "      <td>0.945798</td>\n",
       "      <td>0.946045</td>\n",
       "      <td>0.945863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>0.316958</td>\n",
       "      <td>0.947617</td>\n",
       "      <td>0.948184</td>\n",
       "      <td>0.947617</td>\n",
       "      <td>0.947819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='239' max='239' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [239/239 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for fold 6: {'eval_loss': 0.28173086047172546, 'eval_accuracy': 0.9481403876375065, 'eval_precision': 0.9487130911949786, 'eval_recall': 0.9481403876375065, 'eval_weighted_f1': 0.948239099582891, 'eval_runtime': 5.3194, 'eval_samples_per_second': 358.876, 'eval_steps_per_second': 44.93, 'epoch': 7.998603351955307, 'time_elapsed_secs': 1756.2710304260254}\n",
      "\n",
      "Starting fold 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "All TF 2.0 model weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "C:\\Users\\a\\AppData\\Local\\Temp\\ipykernel_5808\\228697118.py:175: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4290' max='4290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4290/4290 2:36:42, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.459600</td>\n",
       "      <td>0.494261</td>\n",
       "      <td>0.809748</td>\n",
       "      <td>0.855635</td>\n",
       "      <td>0.809748</td>\n",
       "      <td>0.819266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.329600</td>\n",
       "      <td>0.296103</td>\n",
       "      <td>0.893606</td>\n",
       "      <td>0.897098</td>\n",
       "      <td>0.893606</td>\n",
       "      <td>0.894226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.147000</td>\n",
       "      <td>0.237472</td>\n",
       "      <td>0.917191</td>\n",
       "      <td>0.919829</td>\n",
       "      <td>0.917191</td>\n",
       "      <td>0.917986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.204788</td>\n",
       "      <td>0.939727</td>\n",
       "      <td>0.939682</td>\n",
       "      <td>0.939727</td>\n",
       "      <td>0.939244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.129200</td>\n",
       "      <td>0.277056</td>\n",
       "      <td>0.942348</td>\n",
       "      <td>0.942165</td>\n",
       "      <td>0.942348</td>\n",
       "      <td>0.942231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.119000</td>\n",
       "      <td>0.288825</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.944427</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.944344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.048500</td>\n",
       "      <td>0.269726</td>\n",
       "      <td>0.949686</td>\n",
       "      <td>0.949407</td>\n",
       "      <td>0.949686</td>\n",
       "      <td>0.949490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>0.287809</td>\n",
       "      <td>0.949686</td>\n",
       "      <td>0.949505</td>\n",
       "      <td>0.949686</td>\n",
       "      <td>0.949561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.289261</td>\n",
       "      <td>0.957023</td>\n",
       "      <td>0.956768</td>\n",
       "      <td>0.957023</td>\n",
       "      <td>0.956807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='239' max='239' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [239/239 01:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for fold 7: {'eval_loss': 0.2892606854438782, 'eval_accuracy': 0.9570230607966457, 'eval_precision': 0.9567676045610131, 'eval_recall': 0.9570230607966457, 'eval_weighted_f1': 0.9568069749294131, 'eval_runtime': 68.5354, 'eval_samples_per_second': 27.84, 'eval_steps_per_second': 3.487, 'epoch': 9.977653631284916, 'time_elapsed_secs': 9475.37679886818}\n",
      "\n",
      "Starting fold 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "All TF 2.0 model weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "C:\\Users\\a\\AppData\\Local\\Temp\\ipykernel_5808\\228697118.py:175: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4290' max='4290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4290/4290 36:01, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.419200</td>\n",
       "      <td>0.477342</td>\n",
       "      <td>0.833857</td>\n",
       "      <td>0.853325</td>\n",
       "      <td>0.833857</td>\n",
       "      <td>0.838903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.359300</td>\n",
       "      <td>0.336912</td>\n",
       "      <td>0.877883</td>\n",
       "      <td>0.888041</td>\n",
       "      <td>0.877883</td>\n",
       "      <td>0.879718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.219500</td>\n",
       "      <td>0.411549</td>\n",
       "      <td>0.878407</td>\n",
       "      <td>0.900437</td>\n",
       "      <td>0.878407</td>\n",
       "      <td>0.883356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.093000</td>\n",
       "      <td>0.309320</td>\n",
       "      <td>0.918763</td>\n",
       "      <td>0.921440</td>\n",
       "      <td>0.918763</td>\n",
       "      <td>0.919519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.098100</td>\n",
       "      <td>0.380580</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>0.925432</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>0.924902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.021100</td>\n",
       "      <td>0.367486</td>\n",
       "      <td>0.935535</td>\n",
       "      <td>0.934873</td>\n",
       "      <td>0.935535</td>\n",
       "      <td>0.935050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.080700</td>\n",
       "      <td>0.350087</td>\n",
       "      <td>0.939203</td>\n",
       "      <td>0.938713</td>\n",
       "      <td>0.939203</td>\n",
       "      <td>0.938828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>0.393625</td>\n",
       "      <td>0.938155</td>\n",
       "      <td>0.937725</td>\n",
       "      <td>0.938155</td>\n",
       "      <td>0.937772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.398001</td>\n",
       "      <td>0.941300</td>\n",
       "      <td>0.941376</td>\n",
       "      <td>0.941300</td>\n",
       "      <td>0.941293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='239' max='239' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [239/239 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for fold 8: {'eval_loss': 0.39800071716308594, 'eval_accuracy': 0.9412997903563941, 'eval_precision': 0.941376311202224, 'eval_recall': 0.9412997903563941, 'eval_weighted_f1': 0.9412929896215166, 'eval_runtime': 5.2014, 'eval_samples_per_second': 366.824, 'eval_steps_per_second': 45.949, 'epoch': 9.977653631284916, 'time_elapsed_secs': 2171.687174320221}\n",
      "\n",
      "Starting fold 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "All TF 2.0 model weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "C:\\Users\\a\\AppData\\Local\\Temp\\ipykernel_5808\\228697118.py:175: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3870' max='4290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3870/4290 32:01 < 03:28, 2.01 it/s, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.530200</td>\n",
       "      <td>0.522111</td>\n",
       "      <td>0.800314</td>\n",
       "      <td>0.842746</td>\n",
       "      <td>0.800314</td>\n",
       "      <td>0.807293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.370900</td>\n",
       "      <td>0.318466</td>\n",
       "      <td>0.884696</td>\n",
       "      <td>0.883480</td>\n",
       "      <td>0.884696</td>\n",
       "      <td>0.882716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.308727</td>\n",
       "      <td>0.897799</td>\n",
       "      <td>0.902023</td>\n",
       "      <td>0.897799</td>\n",
       "      <td>0.899137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.131900</td>\n",
       "      <td>0.328702</td>\n",
       "      <td>0.909329</td>\n",
       "      <td>0.913788</td>\n",
       "      <td>0.909329</td>\n",
       "      <td>0.910422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.378811</td>\n",
       "      <td>0.918763</td>\n",
       "      <td>0.921733</td>\n",
       "      <td>0.918763</td>\n",
       "      <td>0.919592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.090800</td>\n",
       "      <td>0.330094</td>\n",
       "      <td>0.936583</td>\n",
       "      <td>0.937164</td>\n",
       "      <td>0.936583</td>\n",
       "      <td>0.936747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.104600</td>\n",
       "      <td>0.346972</td>\n",
       "      <td>0.939727</td>\n",
       "      <td>0.939359</td>\n",
       "      <td>0.939727</td>\n",
       "      <td>0.939346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.363171</td>\n",
       "      <td>0.938155</td>\n",
       "      <td>0.938382</td>\n",
       "      <td>0.938155</td>\n",
       "      <td>0.938251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.376176</td>\n",
       "      <td>0.932914</td>\n",
       "      <td>0.933212</td>\n",
       "      <td>0.932914</td>\n",
       "      <td>0.933040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='239' max='239' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [239/239 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for fold 9: {'eval_loss': 0.34697163105010986, 'eval_accuracy': 0.939727463312369, 'eval_precision': 0.9393586954905871, 'eval_recall': 0.939727463312369, 'eval_weighted_f1': 0.9393458638706391, 'eval_runtime': 5.9182, 'eval_samples_per_second': 322.393, 'eval_steps_per_second': 40.384, 'epoch': 9.0, 'time_elapsed_secs': 1932.3676807880402}\n",
      "\n",
      "Starting fold 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "All TF 2.0 model weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "C:\\Users\\a\\AppData\\Local\\Temp\\ipykernel_5808\\228697118.py:175: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4290' max='4290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4290/4290 35:33, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.450900</td>\n",
       "      <td>0.529989</td>\n",
       "      <td>0.811845</td>\n",
       "      <td>0.859058</td>\n",
       "      <td>0.811845</td>\n",
       "      <td>0.822848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>0.303827</td>\n",
       "      <td>0.896226</td>\n",
       "      <td>0.895241</td>\n",
       "      <td>0.896226</td>\n",
       "      <td>0.895048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.188600</td>\n",
       "      <td>0.319373</td>\n",
       "      <td>0.906709</td>\n",
       "      <td>0.911749</td>\n",
       "      <td>0.906709</td>\n",
       "      <td>0.907992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.078400</td>\n",
       "      <td>0.313091</td>\n",
       "      <td>0.924004</td>\n",
       "      <td>0.925043</td>\n",
       "      <td>0.924004</td>\n",
       "      <td>0.924295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.118400</td>\n",
       "      <td>0.316200</td>\n",
       "      <td>0.931866</td>\n",
       "      <td>0.934022</td>\n",
       "      <td>0.931866</td>\n",
       "      <td>0.932597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.437987</td>\n",
       "      <td>0.925577</td>\n",
       "      <td>0.931110</td>\n",
       "      <td>0.925577</td>\n",
       "      <td>0.927039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.322401</td>\n",
       "      <td>0.944969</td>\n",
       "      <td>0.944696</td>\n",
       "      <td>0.944969</td>\n",
       "      <td>0.944750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.339707</td>\n",
       "      <td>0.948113</td>\n",
       "      <td>0.948470</td>\n",
       "      <td>0.948113</td>\n",
       "      <td>0.948222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.340034</td>\n",
       "      <td>0.949686</td>\n",
       "      <td>0.949665</td>\n",
       "      <td>0.949686</td>\n",
       "      <td>0.949642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='239' max='239' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [239/239 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for fold 10: {'eval_loss': 0.3394358158111572, 'eval_accuracy': 0.949685534591195, 'eval_precision': 0.9498381282892528, 'eval_recall': 0.949685534591195, 'eval_weighted_f1': 0.9497263601159663, 'eval_runtime': 5.8361, 'eval_samples_per_second': 326.931, 'eval_steps_per_second': 40.952, 'epoch': 9.977653631284916, 'time_elapsed_secs': 2143.9426476955414}\n",
      "\n",
      "Some folds did not return 'accuracy' metric.\n",
      "\n",
      "Final Cross-Validation Report:\n",
      "Fold 1:\n",
      "  Hyperparameters:\n",
      "    num_train_epochs: 10\n",
      "    per_device_train_batch_size: 8\n",
      "    learning_rate: 3e-05\n",
      "    weight_decay: 0.1\n",
      "  Metrics:\n",
      "    eval_loss: 0.3462\n",
      "    eval_accuracy: 0.9476\n",
      "    eval_precision: 0.9479\n",
      "    eval_recall: 0.9476\n",
      "    eval_weighted_f1: 0.9477\n",
      "    eval_runtime: 6.0227\n",
      "    eval_samples_per_second: 316.9650\n",
      "    eval_steps_per_second: 39.6830\n",
      "    epoch: 9.9986\n",
      "    time_elapsed_secs: 2156.67 seconds\n",
      "\n",
      "Fold 2:\n",
      "  Hyperparameters:\n",
      "    num_train_epochs: 10\n",
      "    per_device_train_batch_size: 8\n",
      "    learning_rate: 3e-05\n",
      "    weight_decay: 0.1\n",
      "  Metrics:\n",
      "    eval_loss: 0.3008\n",
      "    eval_accuracy: 0.9434\n",
      "    eval_precision: 0.9432\n",
      "    eval_recall: 0.9434\n",
      "    eval_weighted_f1: 0.9433\n",
      "    eval_runtime: 31.1055\n",
      "    eval_samples_per_second: 61.3720\n",
      "    eval_steps_per_second: 7.6840\n",
      "    epoch: 8.9986\n",
      "    time_elapsed_secs: 4073.78 seconds\n",
      "\n",
      "Fold 3:\n",
      "  Hyperparameters:\n",
      "    num_train_epochs: 10\n",
      "    per_device_train_batch_size: 8\n",
      "    learning_rate: 3e-05\n",
      "    weight_decay: 0.1\n",
      "  Metrics:\n",
      "    eval_loss: 0.2861\n",
      "    eval_accuracy: 0.9544\n",
      "    eval_precision: 0.9542\n",
      "    eval_recall: 0.9544\n",
      "    eval_weighted_f1: 0.9542\n",
      "    eval_runtime: 5.5486\n",
      "    eval_samples_per_second: 344.0530\n",
      "    eval_steps_per_second: 43.0740\n",
      "    epoch: 9.9986\n",
      "    time_elapsed_secs: 2134.24 seconds\n",
      "\n",
      "Fold 4:\n",
      "  Hyperparameters:\n",
      "    num_train_epochs: 10\n",
      "    per_device_train_batch_size: 8\n",
      "    learning_rate: 3e-05\n",
      "    weight_decay: 0.1\n",
      "  Metrics:\n",
      "    eval_loss: 0.3440\n",
      "    eval_accuracy: 0.9481\n",
      "    eval_precision: 0.9481\n",
      "    eval_recall: 0.9481\n",
      "    eval_weighted_f1: 0.9481\n",
      "    eval_runtime: 5.1890\n",
      "    eval_samples_per_second: 367.8960\n",
      "    eval_steps_per_second: 46.0590\n",
      "    epoch: 9.9986\n",
      "    time_elapsed_secs: 2139.80 seconds\n",
      "\n",
      "Fold 5:\n",
      "  Hyperparameters:\n",
      "    num_train_epochs: 10\n",
      "    per_device_train_batch_size: 8\n",
      "    learning_rate: 3e-05\n",
      "    weight_decay: 0.1\n",
      "  Metrics:\n",
      "    eval_loss: 0.2989\n",
      "    eval_accuracy: 0.9534\n",
      "    eval_precision: 0.9537\n",
      "    eval_recall: 0.9534\n",
      "    eval_weighted_f1: 0.9535\n",
      "    eval_runtime: 44.9112\n",
      "    eval_samples_per_second: 42.5060\n",
      "    eval_steps_per_second: 5.3220\n",
      "    epoch: 9.9986\n",
      "    time_elapsed_secs: 3073.02 seconds\n",
      "\n",
      "Fold 6:\n",
      "  Hyperparameters:\n",
      "    num_train_epochs: 10\n",
      "    per_device_train_batch_size: 8\n",
      "    learning_rate: 3e-05\n",
      "    weight_decay: 0.1\n",
      "  Metrics:\n",
      "    eval_loss: 0.2817\n",
      "    eval_accuracy: 0.9481\n",
      "    eval_precision: 0.9487\n",
      "    eval_recall: 0.9481\n",
      "    eval_weighted_f1: 0.9482\n",
      "    eval_runtime: 5.3194\n",
      "    eval_samples_per_second: 358.8760\n",
      "    eval_steps_per_second: 44.9300\n",
      "    epoch: 7.9986\n",
      "    time_elapsed_secs: 1756.27 seconds\n",
      "\n",
      "Fold 7:\n",
      "  Hyperparameters:\n",
      "    num_train_epochs: 10\n",
      "    per_device_train_batch_size: 8\n",
      "    learning_rate: 3e-05\n",
      "    weight_decay: 0.1\n",
      "  Metrics:\n",
      "    eval_loss: 0.2893\n",
      "    eval_accuracy: 0.9570\n",
      "    eval_precision: 0.9568\n",
      "    eval_recall: 0.9570\n",
      "    eval_weighted_f1: 0.9568\n",
      "    eval_runtime: 68.5354\n",
      "    eval_samples_per_second: 27.8400\n",
      "    eval_steps_per_second: 3.4870\n",
      "    epoch: 9.9777\n",
      "    time_elapsed_secs: 9475.38 seconds\n",
      "\n",
      "Fold 8:\n",
      "  Hyperparameters:\n",
      "    num_train_epochs: 10\n",
      "    per_device_train_batch_size: 8\n",
      "    learning_rate: 3e-05\n",
      "    weight_decay: 0.1\n",
      "  Metrics:\n",
      "    eval_loss: 0.3980\n",
      "    eval_accuracy: 0.9413\n",
      "    eval_precision: 0.9414\n",
      "    eval_recall: 0.9413\n",
      "    eval_weighted_f1: 0.9413\n",
      "    eval_runtime: 5.2014\n",
      "    eval_samples_per_second: 366.8240\n",
      "    eval_steps_per_second: 45.9490\n",
      "    epoch: 9.9777\n",
      "    time_elapsed_secs: 2171.69 seconds\n",
      "\n",
      "Fold 9:\n",
      "  Hyperparameters:\n",
      "    num_train_epochs: 10\n",
      "    per_device_train_batch_size: 8\n",
      "    learning_rate: 3e-05\n",
      "    weight_decay: 0.1\n",
      "  Metrics:\n",
      "    eval_loss: 0.3470\n",
      "    eval_accuracy: 0.9397\n",
      "    eval_precision: 0.9394\n",
      "    eval_recall: 0.9397\n",
      "    eval_weighted_f1: 0.9393\n",
      "    eval_runtime: 5.9182\n",
      "    eval_samples_per_second: 322.3930\n",
      "    eval_steps_per_second: 40.3840\n",
      "    epoch: 9.0000\n",
      "    time_elapsed_secs: 1932.37 seconds\n",
      "\n",
      "Fold 10:\n",
      "  Hyperparameters:\n",
      "    num_train_epochs: 10\n",
      "    per_device_train_batch_size: 8\n",
      "    learning_rate: 3e-05\n",
      "    weight_decay: 0.1\n",
      "  Metrics:\n",
      "    eval_loss: 0.3394\n",
      "    eval_accuracy: 0.9497\n",
      "    eval_precision: 0.9498\n",
      "    eval_recall: 0.9497\n",
      "    eval_weighted_f1: 0.9497\n",
      "    eval_runtime: 5.8361\n",
      "    eval_samples_per_second: 326.9310\n",
      "    eval_steps_per_second: 40.9520\n",
      "    epoch: 9.9777\n",
      "    time_elapsed_secs: 2143.94 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig, TrainingArguments, Trainer, DataCollatorWithPadding, EarlyStoppingCallback, AutoModelForSequenceClassification\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 1. Read CSV file and basic preprocessing\n",
    "# ----------------------------------------------------\n",
    "df = pd.read_csv(\"output_file2.csv\")\n",
    "df[\"text\"] = df[\"text\"].str.replace(r\"http\\S+\", \"\", regex=True)\n",
    "df['label'] = df['label'].replace(2, 1)\n",
    "df['label'] = df['label'].replace(3, 2)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 2. Automatically compute class weights\n",
    "# ----------------------------------------------------\n",
    "label_counts = df['label'].value_counts().sort_index()  # Assuming sorted by label value\n",
    "print(\"Label counts:\\n\", label_counts)\n",
    "\n",
    "total_samples = len(df)\n",
    "num_classes = len(label_counts)\n",
    "computed_weights = total_samples / (num_classes * label_counts)\n",
    "print(\"Computed class weights:\\n\", computed_weights)\n",
    "\n",
    "class_weights = torch.tensor(computed_weights.values, dtype=torch.float)\n",
    "print(\"Tensor class weights:\", class_weights)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 3. Load model, configuration, and tokenizer\n",
    "# ----------------------------------------------------\n",
    "model_checkpoint = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "config = RobertaConfig.from_pretrained(model_checkpoint)\n",
    "config.num_labels = num_classes\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    config=config,\n",
    "    from_tf=True,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(\"Classifier layer shape:\", model.classifier.out_proj.weight.shape)\n",
    "\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 4. Tokenize the dataset\n",
    "# ----------------------------------------------------\n",
    "def tokenize_function(examples):\n",
    "    texts = [str(text) for text in examples[\"text\"]]\n",
    "    return tokenizer(texts, truncation=True, max_length=512)\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 5. Create custom Trainer with weighted loss\n",
    "# ----------------------------------------------------\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        # Ensure the class weights are on the same device as the logits\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights.to(logits.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 6. Metrics computation for evaluation\n",
    "# ----------------------------------------------------\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    precision = precision_score(labels, predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(labels, predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(labels, predictions, average='weighted', zero_division=0)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"weighted_f1\": f1,\n",
    "    }\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Step 7. Setup Stratified K-Fold Cross-Validation and training\n",
    "# ----------------------------------------------------\n",
    "n_splits = 10\n",
    "\n",
    "labels = df['label'].values\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "fold_metrics = {}\n",
    "results_report = [] \n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "print(\"Starting 10-fold Cross-Validation with StratifiedKFold...\\n\")\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(np.zeros(len(labels)), labels), start=1):\n",
    "    print(f\"Starting fold {fold}...\")\n",
    "    start_fold_time = time.time()\n",
    "\n",
    "    train_split = tokenized_dataset.select(train_index.tolist())\n",
    "    val_split = tokenized_dataset.select(test_index.tolist())\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=10,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=3e-05,\n",
    "        weight_decay=0.1,\n",
    "        logging_steps=10,\n",
    "        disable_tqdm=False,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_weighted_f1\",\n",
    "        greater_is_better=True,\n",
    "        no_cuda=False,\n",
    "        warmup_steps=500,\n",
    "        gradient_accumulation_steps=5,\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.999,\n",
    "        adam_epsilon=1e-8,\n",
    "        max_grad_norm=1.0,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        logging_dir='./logs',\n",
    "        run_name=\"roberta_weighted_loss_stratified\",\n",
    "        save_total_limit=3,\n",
    "    )\n",
    "\n",
    "    fold_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        num_labels=num_classes,\n",
    "        from_tf=True\n",
    "    ).to(device)\n",
    "    \n",
    "    trainer = WeightedLossTrainer(\n",
    "        model=fold_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_split,\n",
    "        eval_dataset=val_split,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "\n",
    "    end_fold_time = time.time()\n",
    "    time_elapsed = end_fold_time - start_fold_time\n",
    "    metrics[\"time_elapsed_secs\"] = time_elapsed\n",
    "\n",
    "    print(f\"Metrics for fold {fold}: {metrics}\\n\")\n",
    "    fold_metrics[fold] = metrics\n",
    "\n",
    "    fold_report = {\n",
    "        \"fold\": fold,\n",
    "        \"hyperparameters\": {\n",
    "            \"num_train_epochs\": training_args.num_train_epochs,\n",
    "            \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "            \"learning_rate\": training_args.learning_rate,\n",
    "            \"weight_decay\": training_args.weight_decay,\n",
    "        },\n",
    "        \"metrics\": metrics\n",
    "    }\n",
    "    results_report.append(fold_report)\n",
    "\n",
    "    trainer.model.save_pretrained(f\"./binary_roberta_final_fold_{fold}\")\n",
    "    tokenizer.save_pretrained(f\"./binary_roberta_fold_{fold}\")\n",
    "\n",
    "trainer.save_model(\"./final_finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./final_finetuned_model\")\n",
    "if all(\"accuracy\" in fold_metrics[f] for f in fold_metrics):\n",
    "    avg_accuracy = np.mean([fold_metrics[f][\"accuracy\"] for f in fold_metrics])\n",
    "    print(f\"Average eval accuracy over {n_splits} folds: {avg_accuracy:.4f}\")\n",
    "else:\n",
    "    print(\"Some folds did not return 'accuracy' metric.\")\n",
    "\n",
    "print(\"\\nFinal Cross-Validation Report:\")\n",
    "for fold_report in results_report:\n",
    "    print(f\"Fold {fold_report['fold']}:\")\n",
    "    print(\"  Hyperparameters:\")\n",
    "    for hp, val in fold_report[\"hyperparameters\"].items():\n",
    "        print(f\"    {hp}: {val}\")\n",
    "    print(\"  Metrics:\")\n",
    "    for metric, value in fold_report[\"metrics\"].items():\n",
    "        if metric == \"time_elapsed_secs\":\n",
    "            print(f\"    {metric}: {value:.2f} seconds\")\n",
    "        else:\n",
    "            print(f\"    {metric}: {value:.4f}\")\n",
    "    print(\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
